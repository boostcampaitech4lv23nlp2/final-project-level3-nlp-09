{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataset import FoodImageDataset, get_split_dataset\n",
    "from model import build_model\n",
    "from preprocess import image_transform\n",
    "from tokenizer import FoodTokenizer\n",
    "\n",
    "with open(\"model_configs/baseline.json\") as f:\n",
    "    configs = json.load(f)\n",
    "text_cfg = configs[\"text_cfg\"]\n",
    "vision_cfg = configs[\"vision_cfg\"]\n",
    "\n",
    "model = build_model(vision_cfg, text_cfg)\n",
    "checkpoint = torch.load('/opt/ml/final-project-level3-nlp-09/src/output/01161654_epochs-10_batch-128/epoch_9.pt', map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class FoodImageDataset(Dataset):\n",
    "    def __init__(self, transforms, mode=\"train\"):\n",
    "        #self.args = args\n",
    "        self.dataset_path = \"../data\"\n",
    "        self.dataset_mode = \"train\" if mode == \"train\" else \"test\"\n",
    "        self.labels_info_file_name = \"labels.json\"\n",
    "        self.train_info_file_name = \"aihub_1.0_43_0.3_train_crop.json\"\n",
    "        self.test_info_file_name = \"aihub_1.0_43_0.3_test_crop.json\"\n",
    "        self.labels_file_path = os.path.join(self.dataset_path, self.labels_info_file_name)\n",
    "        self.train_file_path = os.path.join(self.dataset_path, self.train_info_file_name)\n",
    "        self.test_file_path = os.path.join(self.dataset_path, self.test_info_file_name)\n",
    "\n",
    "        self.label_data = None\n",
    "        self.train_data = None\n",
    "        self.id_to_text_dict = None\n",
    "        self.text_to_id_dict = None\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.labels, self.data = self.get_dataset(self.labels_file_path, self.train_file_path)\n",
    "        elif mode == \"test\":\n",
    "            self.labels, self.data = self.get_dataset(self.labels_file_path, self.test_file_path)\n",
    "\n",
    "        self.id_to_text_dict = self.get_id_to_text(self.labels)\n",
    "        self.text_to_id_dict = self.get_text_to_id(self.labels)\n",
    "\n",
    "        self.data = self.data\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_dataset(self, labels_file_path, data_file_path):\n",
    "        with open(labels_file_path, \"r\") as file:\n",
    "            labels = json.load(file)\n",
    "            labels = labels[\"categories\"]\n",
    "\n",
    "        with open(data_file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "            data = data[\"images\"]\n",
    "\n",
    "        return labels, data\n",
    "\n",
    "    def get_id_to_text(self, label_data):\n",
    "        return {item[\"id\"]: item[\"label\"] for item in label_data}\n",
    "\n",
    "    def get_text_to_id(self, label_data):\n",
    "        return {item[\"label\"]: item[\"id\"] for item in label_data}\n",
    "\n",
    "    def transform_func(self, examples):\n",
    "        examples[\"image\"] = [self.preprocess(image) for image in examples[\"image\"]]\n",
    "        return examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text_id = self.data[idx][\"category_id\"]\n",
    "        text = self.id_to_text_dict[text_id]\n",
    "        file_name = os.path.split(self.data[idx][\"file_name\"])[-1]\n",
    "        file_path = os.path.join(self.dataset_path, self.dataset_mode, file_name)\n",
    "        image = Image.open(file_path)\n",
    "        image = self.transforms(image)\n",
    "        return text, image\n",
    "\n",
    "\n",
    "def get_split_dataset(dataset, ratio):\n",
    "    dataset_a_len = int(len(dataset) * ratio)\n",
    "    dataset_b_len = int(len(dataset) - dataset_a_len)\n",
    "    dataset_a, dataset_b = random_split(dataset, [dataset_a_len, dataset_b_len])\n",
    "    return dataset_a, dataset_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = image_transform(vision_cfg[\"image_size\"], is_train=True)\n",
    "\n",
    "train_dataset = FoodImageDataset(preprocess, mode=\"train\")\n",
    "dataset = FoodImageDataset(preprocess, mode=\"test\")\n",
    "valid_dataset, test_dataset = get_split_dataset(dataset, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/632 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to() received an invalid combination of arguments - got (str, dtype=str), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m texts, images \u001b[39min\u001b[39;00m tqdm(valid_dataloader):\n\u001b[0;32m---> 15\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mto(device, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfp16\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m     images_encoded \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_image(images)\n\u001b[1;32m     18\u001b[0m     all_images\u001b[39m.\u001b[39mappend(images_encoded)\n",
      "\u001b[0;31mTypeError\u001b[0m: to() received an invalid combination of arguments - got (str, dtype=str), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_images = []\n",
    "all_texts = []\n",
    "\n",
    "with open(\"../data/labels.json\", \"r\") as file:\n",
    "        labels = json.load(file)\n",
    "        labels = labels[\"categories\"]\n",
    "\n",
    "indices = {item[\"label\"]: item[\"id\"] for item in labels}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "for texts, images in tqdm(valid_dataloader):\n",
    "        with autocast():\n",
    "                images = images.to(device, dtype='fp16')\n",
    "                images_encoded = model.encode_image(images)\n",
    "\n",
    "                all_images.append(images_encoded)\n",
    "                \n",
    "                texts = [indices[item] for item in texts]\n",
    "                all_texts.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
